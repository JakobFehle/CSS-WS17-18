{
    "collab_server" : "",
    "contents" : "require(tidyverse)\nrequire(dplyr)\nrequire(plyr)\nlibrary(readr)\nlibrary(SentimentAnalysis)\nlibrary(SnowballC)\nlibrary(tm)\nlibrary(spikeslab)\nsource(\"r scripts/cleanTweetText.R\")\n\n\n\ntwitter_data<-read_csv(\"twitter data/twitter_data.csv\", \n                        locale = locale())\n\n# Aufbereiten Twitter Datensatz\ntwitter_data<-twitter_data%>%filter(lang==\"de\" | lang == \"da\")\n\n#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)\ntwitter_data$text<-gsub(\"http[s]?://t\\\\.co/[^ ]{10}\",\"\",twitter_data$text)\ntwitter_data$text<-gsub(\"http.*[^\\\\s]+\",\"\",twitter_data$text)\ntwitter_data$text<-gsub(\"https\",\"\",twitter_data$text)\ntwitter_data$text<-gsub(\"http\",\"\",twitter_data$text)\n\n\n#Entfernt alle abgeschnittenen Links\ntwitter_data$text<-gsub(\"htt[p]?\\U2026\",\"\",twitter_data$text)\n\n#Entfernen von abgeschnittenen Tweets\ntwitter_data<-twitter_data[!grepl(\"\\U2026\", twitter_data$text),]\n\n#CleanTweetText auf den gesamten Twitter Datensatz\ntwitter_data<-twitter_data%>%mutate(text=cleanTweetText(text))\n\n# Aufbereiten für Random Sample\ntwitter_data<-twitter_data%>%mutate(sentimentScore=NA)\n\n# NUR FÜR EXCEL!\ntwitter_data$text<-gsub(\",\",\" \",twitter_data$text)\ntwitter_data$text<-gsub(\";\",\" \",twitter_data$text)\n\n# Random Sample\ntwitter_data_frac<-twitter_data%>%sample_n(1000, replace=FALSE)\ntwitter_data_frac<-twitter_data_frac%>%select(ID,sentimentScore,text)\n\nwrite.table(twitter_data_frac, \"twitter data/Twitter_Sentiment_TrainData.csv\", sep = \",\")\n\n\n## Auswertung\n\n# Einlesen\ntwitter_data_sentiment_david<-read_delim(\"twitter data/SentimentAnalyseDavid.csv\", \n                                   \";\", escape_double = FALSE, trim_ws = TRUE)\n\ntwitter_data_sentiment_jakob<-read_delim(\"twitter data/SentimentAnalyseJakobMatchedFromDavid.csv\", \n                                         \";\", escape_double = FALSE, trim_ws = TRUE)\ncolnames(twitter_data_sentiment_jakob)<-c(\"Nummer\",\"ID\",\"isMatch\",\"sentimentScore\",\"text\")\ntwitter_data_sentiment_jakob<-twitter_data_sentiment_jakob[,c(\"Nummer\",\"ID\",\"sentimentScore\",\"isMatch\",\"text\")]\n\ntwitter_data_sentiment<-rbind(twitter_data_sentiment_david,twitter_data_sentiment_jakob)\n\n# Spalten Säubern (ID leider verfälscht)\ntwitter_data_sentiment<-twitter_data_sentiment%>%select(-c(Nummer,ID))\n\n# Säubern\ntwitter_data_sentiment$text<-gsub(\"http[s]?\\\\://t\\\\.co/[^ ]{10}\",\"\",twitter_data_sentiment$text)\ntwitter_data_sentiment$text<-gsub(\"http.*[^\\\\s]+\",\"\",twitter_data_sentiment$text)\ntwitter_data_sentiment$text<-gsub(\"htt[p]?\\U2026\",\"\",twitter_data_sentiment$text)\ntwitter_data_sentiment$text<-gsub(\"https\",\"\",twitter_data_sentiment$text)\ntwitter_data_sentiment$text<-gsub(\"http\",\"\",twitter_data_sentiment$text)\ntwitter_data_sentiment<-twitter_data_sentiment[!grepl(\"\\U2026\", twitter_data_sentiment$text),]\n\n# Fehlerhaft bei ä,ü,ö\ntwitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))\n\n# Nur Matches der Kreuzvalid.\ntwitter_data_sentiment_match<-twitter_data_sentiment%>%filter(isMatch ==\"1\")\n\n# Umwandeln der Scores in numeric mit \".\"\ntwitter_data_sentiment_match<-twitter_data_sentiment_match%>%mutate(sentimentScore = sub(\",\", \".\", sentimentScore, fixed = TRUE))\ntwitter_data_sentiment_match[,\"sentimentScore\"]<-sapply(twitter_data_sentiment_match[,\"sentimentScore\"],as.numeric)\ntwitter_data_sentiment_match<-twitter_data_sentiment_match%>%filter(sentimentScore != \"NA\")\ntwitter_data_sentiment_match<-twitter_data_sentiment_match%>%filter(sentimentScore != 0)\ntwitter_data_sentiment_match<-twitter_data_sentiment_match%>%mutate(sentimentScore = sub(\"0.5\",\"1\", sentimentScore, fixed = TRUE))\n# Testweise join für orginalen Text\n# twitter_data_merge<-merge(x = twitter_data, y = twitter_data_sentiment, by = \"text\", all.x = TRUE)\n\n\nx<-transformIntoCorpus(twitter_data_sentiment_match$text)\nresponse<-as.numeric(as.character(twitter_data_sentiment_match$sentimentScore))\n\ndict_lasso_pol<-generateDictionary(xDM,response,modelType = \"lasso\", filterTerms = NULL, control = list(), sparsity = 0.99999999, weighting= function(x)\n                           tm::weightTfIdf(x, normalize = TRUE), language = \"german\")\n\ndict_lm_pol<-generateDictionary(x,response,modelType = \"lm\", filterTerms = NULL, control = list(), sparsity = 0.99999999, weighting= function(x)\n                              tm::weightTfIdf(x, normalize = TRUE), language = \"german\")\n\ndict_enet_pol<-generateDictionary(x,response,modelType = \"enet\", filterTerms = NULL, control = list(), sparsity = 0.99999999, weighting= function(x)\n  tm::weightTfIdf(x, normalize = TRUE), language = \"german\")\n\ndict_ridge_pol<-generateDictionary(x,response,modelType = \"ridge\", filterTerms = NULL, control = list(), sparsity = 0.99999999, weighting= function(x)\n  tm::weightTfIdf(x, normalize = TRUE), language = \"german\")\n\nsummary(dict_lasso_pol)\nplot(dict_lasso_pol)\nplot(dict_lm_pol)\nplot(dict_enet_pol)\nplot(dict_ridge_pol)\n\n# Shit\n#twitter_data_exp<-twitter_data%>%select(c(\"ID\",\"positveSentimentScore\",\"negativeSentimentScore\",\"text\"))\n#twitter_data_exp$sentimentScore<-\"\"\n#twitter_data_exp<-twitter_data_exp[c(\"ID\",\"positveSentimentScore\",\"negativeSentimentScore\",\"sentimentScore\",\"text\")]\n\n#twitter_data_exp<-twitter_data_exp%>%arrange(desc(positveSentimentScore),desc(negativeSentimentScore))\n#twitter_data_exp_pos<-head(twitter_data_exp,200)\n#twitter_data_exp_pos<-twitter_data_exp[1:400,]\n#write.csv(twitter_data_exp_pos, \"twitter data/ExampleTweetsSentiStrengthPos.csv\")\n\n#twitter_data_exp<-twitter_data_exp%>%arrange(negativeSentimentScore,positveSentimentScore)\n#twitter_data_exp_neg<-head(twitter_data_exp,200)\n#write.csv(twitter_data_exp_neg, \"twitter data/ExampleTweetsSentiStrengthNeg.csv\")\n#twitter_data_exp_neg<-twitter_data_exp[201:400,]\n#write.csv(twitter_data_exp_neg, \"twitter data/ExampleTweetsSentiStrengthNeg2.csv\")\n\n\n# Umgehen von Stemming\nxDM<-toDocumentTermMatrix(x, language = \"german\", minWordLength = 3,\n                     sparsity = NULL, removeStopwords = TRUE, stemming = FALSE,\n                     weighting = function(x) tm::weightTfIdf(x, normalize = FALSE))\n\n# Korrekte dekodierung von <FC> etc in ü, ä, ö!\n",
    "created" : 1512916653140.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4008444410",
    "id" : "4A974DBA",
    "lastKnownWriteTime" : 1513739161,
    "last_content_update" : 1513739161362,
    "path" : "D:/GitHub/CSS-WS17-18/r scripts/sentimentSample.R",
    "project_path" : "r scripts/sentimentSample.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}