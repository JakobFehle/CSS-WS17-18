<<<<<<< HEAD
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTextForMerge("<U+6594>")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data_sentiment<-read_delim("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%select(-c(X1,ID,positveSentimentScore,negativeSentimentScore))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%filter(sentimentScore != "NA")
twitter_data_sentiment_jakob_snd$isMatch<-"1"
twitter_data_sentiment<-twitter_data_sentiment%>%select(-c(ID))
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd)
twitter_data_sentiment<-twitter_data_sentiment%>%filter(isMatch=="1")
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))
twitter_data_sentiment<-twitter_data_sentiment%>%filter(text!="")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data<-twitter_data%>%filter(text != "")
twitter_data_sentiment22<-join(x=twitter_data_sentiment,y=twitter_data, type = "inner")
twitter_data_sentiment22<-twitter_data_sentiment22[!duplicated((twitter_data_sentiment22$text)),]
twitter_data_sentiment_anti<-anti_join(x=twitter_data_sentiment,y=twitter_data)
View(twitter_data_sentiment_anti)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTweetText(".@c_lindner ungeschminkt. Christian #Lindner Unterhemd.https://t.co/")
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("\U00E3","ä",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
text<-gsub("\\s+", " ",text)
return(text)
}
cleanTextForMerge<-function(text){
mention_pattern <- "@([[:alnum:]]|[_])+"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-gsub("\U2026","",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTextForMerge("<U+6594>")
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data_sentiment<-read_delim("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%select(-c(X1,ID,positveSentimentScore,negativeSentimentScore))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%filter(sentimentScore != "NA")
twitter_data_sentiment_jakob_snd$isMatch<-"1"
twitter_data_sentiment<-twitter_data_sentiment%>%select(-c(ID))
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd)
twitter_data_sentiment<-twitter_data_sentiment%>%filter(isMatch=="1")
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))
twitter_data_sentiment<-twitter_data_sentiment%>%filter(text!="")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data<-twitter_data%>%filter(text != "")
twitter_data_sentiment22<-join(x=twitter_data_sentiment,y=twitter_data, type = "inner")
twitter_data_sentiment22<-twitter_data_sentiment22[!duplicated((twitter_data_sentiment22$text)),]
View(twitter_data_sentiment_anti)
cleanTweetText("gute amp richtige entscheidung")
Encoding(twitter_data_sentiment_anti[1,])
twitter_data_sentiment_anti[1,]
Encoding(twitter_data_sentiment_anti[1,3])
twitter_data_sentiment_anti[1,3]
toxt<-twitter_data_sentiment_anti[1,3]
Encoding(toxt)
Encoding(as.character(toxt))
twitter_data_sentiment<-read_csv2("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE, encoding="UTF-8")
twitter_data_sentiment<-read_csv2("twitter data/SentimentAnalyse#1.csv",";", encoding="UTF-8")
twitter_data_sentiment<-read.csv2("twitter data/SentimentAnalyse#1.csv",";", encoding="UTF-8")
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,ID,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
twitter_data_sentiment22<-twitter_data_sentiment22%>%select(c(ID,sentimentScore))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_sentiment22)
View(twitter_data2)
twitter_data<-read_delim("twitter data/twitter_data.csv",
",", escape_double = FALSE, trim_ws = TRUE,
locale = locale())
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
View(twitter_data2)
twitter_data2<-read_delim("twitter data/twitter_data.csv",
",", escape_double = FALSE, trim_ws = TRUE,
locale = locale())
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
lasso_5lvl<-read("dictionarys/lasso-5level.dict")
lasso_3lvl<-read("dictionarys/lasso-3level.dict")
lasso_2lvl<-read("dictionarys/lasso-2level.dict")
ridge_5lvl<-read("dictionarys/ridge-5level.dict")
ridge_3lvl<-read("dictionarys/ridge-3level.dict")
ridge_2lvl<-read("dictionarys/ridge-2level.dict")
enet_5lvl<-read("dictionarys/enet-5level.dict")
enet_3lvl<-read("dictionarys/enet-3level.dict")
enet_2lvl<-read("dictionarys/enet-2level.dict")
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
View(twitter_data_WS_join)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
#text<-substr(text, 1, 30)
return(text)
}
cleanTweetText("gute amp richtige entscheidung")
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("\U00E3","ä",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
=======
>>>>>>> 55cc94dd0e279e6579ba715551e2f945fb3f362d
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
text<-gsub("\\s+", " ",text)
return(text)
}
cleanTextForMerge<-function(text){
mention_pattern <- "@([[:alnum:]]|[_])+"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-gsub("\U2026","",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
#text<-substr(text, 1, 30)
return(text)
}
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
View(twitter_data_WS_join)
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
View(twitter_data_WS_join)
twitter_data_WS_join_text<-twitter_data_WS_join$text
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join_text<-twitter_data_WS_join[,"text"]
twitter_data_WS_join_text$text<-twitter_data_WS_join[,"text"]
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text$text,1,function(x) stemTweetText(x))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-lapply(twitter_data_WS_join_text$text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],function(x) stemTweetText(x))
lasso5<-predict(lasso_5lvl,twitter_data_WS_join$text)
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
View(twitter_data_WS_join)
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
lasso_5lvl<-read("dictionarys/lasso-5level.dict")
lasso_3lvl<-read("dictionarys/lasso-3level.dict")
lasso_2lvl<-read("dictionarys/lasso-2level.dict")
ridge_5lvl<-read("dictionarys/ridge-5level.dict")
ridge_3lvl<-read("dictionarys/ridge-3level.dict")
ridge_2lvl<-read("dictionarys/ridge-2level.dict")
enet_5lvl<-read("dictionarys/enet-5level.dict")
enet_3lvl<-read("dictionarys/enet-3level.dict")
enet_2lvl<-read("dictionarys/enet-2level.dict")
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
twitter_data_WS_join<-as.data.frame(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
twitter_data_WS_join<-as.data.frame(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],function(x) stemTweetText(x))
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,unlist(twitter_data_WS_join$text))))
predict(enet_5lvl,"vorsprung durch leckmich kartell")
View(twitter_data_WS)
as.numeric(predict(enet_5lvl,"vorsprung durch leckmich kartell"))
twitter_data_WS<-twitter_data_WS%>%select(-c(isRetweet,isQuote))
View(twitter_data2)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
View(twitter_data)
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
<<<<<<< HEAD
View(twitter_data)
twitter_data<-twitter_data%>%select(c(text,ID))
View(twitter_data)
twitter_data_WS<-join(x=twitter_data_WS,y=twitter_data,type="inner",by="text")
View(twitter_data_WS)
library(readr)
twitterDataWithSentiment <- read_delim("D:/GitHub/CSS-WS17-18/twitter data/twitterDataWithSentiment.csv",
";", escape_double = FALSE, locale = locale(),
trim_ws = TRUE)
View(twitterDataWithSentiment)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
View(twitter_data)
View(twitterDataWithSentiment)
twitterDataWithSentiment$ID<-twitter_data$ID
moveme(names(twitterDataWithSentiment),"ID first")
twitterDataWithSentiment<-twitterDataWithSentiment%>%select(ID,everything())
View(twitterDataWithSentiment)
View(twitter_data_WS_join)
write.csv(twitter_data_WS_join, "twitter data/twitterDataMatches.csv",sep=",", col.names = TRUE, row.names = FALSE)
write.table(twitter_data_WS_join, "twitter data/twitterDataMatches.csv", sep=",",col.names = TRUE, row.names = FALSE)
str(twitter_data_WS_join)
twitter_data_WS_join$text<-unlist(twitter_data_WS_join$text)
write.table(twitter_data_WS_join, "twitter data/twitterDataMatches.csv", sep=",",col.names = TRUE, row.names = FALSE)
twitter_data_WS_join<-twitter_data_WS_join%>%select(ID,sentimentScore)
twitterDataWithSentiment2<-join(x=twitterDataWithSentiment,twitter_data_WS_join,type="inner",by="ID")
twitterDataWithSentiment2<-join(x=twitterDataWithSentiment,twitter_data_WS_join,type="left",by="ID")
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
twitterDataWS<-twitterDataWithSentiment2%>%filter(sentimentScore!="NA")
View(twitterDataWS)
twitterDataWS<-twitterDataWS[!duplicated((twitterDataWS$text)),]
write.table(twitterDataWS, "twitter data/twitterDataBaselineWithAllDicts.csv", sep=",",col.names = TRUE, row.names = FALSE)
=======
twitter_data_cleaned$text<-twitter_data_cleaned$text<-iconv(twitter_data_cleaned$text, 'UTF-8','ASCII')
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
# Stemming
twitter_data_cleaned$text<-apply(twitter_data_cleaned[,"text"],1,function(x) stemTweetText(x))
twitter_data$lasso5<-as.numeric(unlist(predict))
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUE)
predict<-predict(ridge_5lvl,twitter_data_cleaned$text)
twitter_data1<-twitter_data[1:120000,]
twitter_data_cleaned1<-twitter_data_cleaned[1:120000,]
predict1<-predict(ridge_5lvl,twitter_data_cleaned1$text)
setwd("C:/Users/David/Downloads/Uni/CSS/projekt/CSS-WS17-18-master/r scripts")
require(tidyverse)
require(ggraph2)
library(gridExtra)
library(grid)
source('scripts/cleanHashtags.R')
source('cleanHashtags.R')
setwd("C:/Users/David/Downloads/Uni/CSS/projekt/CSS-WS17-18-master")
source('scripts/cleanHashtags.R')
source('r scripts/cleanTweetText.R')
source('r scripts/cleanHashtags.R')
twitter_data_sample<-read_csv("data/twitter_data_sample.csv",
locale = locale())
twitter_data_sample<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Anzahl Tweets als Retweets
twitter_data_retweets<-twitter_data_sample%>%filter(isRetweet==1)
nrow(twitter_data_retweets)
nrow(twitter_data_sample)
nrow(twitter_data_retweets)
nrow(twitter_data_quotes)
# Anzahl Tweets als Quotes
twitter_data_quotes<-twitter_data_sample%>%filter(isQuote==1)
nrow(twitter_data_quotes)
# Anzahl Tweets als Antworten auf andere Tweets
twitter_data_replies<-twitter_data_sample%>%filter(inReplyToUser!='null')
nrow(twitter_data_replies)
ggplot(twitter_data_quotes,twitter_data_replies,twitter_data_retweets)
ggplot(twitter_data_quotes,twitter_data_replies,twitter_data_retweets+aes())
ggplot(twitter_data_quotes,twitter_data_replies,twitter_data_retweets)+aes()
pplot <-ggplot(ddf,x=group, y=mean, aes(group,mean,fill=time))
pplot <-ggplot(twitter_data_sample%>%filter(inReplyToUser!='null'),x=group, y=mean, aes(group,mean,fill=time))
pplot +
geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymax = mean + se, ymin= mean - se), position = position_dodge(width=0.7), width=0.2) +
#geom_point(stat="identity", shape=21, size=5, position=position_dodge(width=0.7), width=0.2)
geom_point(stat="identity", shape=21, size=5, position=position_dodge(width=0.7))
p <-ggplot(twitter_data_sample%>%filter(isQuote==1), aes(class))
p + geom_bar()
p <-ggplot(twitter_data_sample%>%filter(isQuote==1), aes(isRetweet))
p + geom_bar()
nrow(twitter_data_retweets)
basic <-c("Retweet","Quotes","Answer")
basic
nrow(twitter_data_retweets)
values <-c(twitter_data_retweets,twitter_data_quotes,twitter_data_replies)
values
values <-c(nrow(twitter_data_retweets),nrow(twitter_data_quotes),nrow(twitter_data_replies))
values
basic_plot_data<-data.frame(basic,values)
basic_plot <- ggplot(basic_plot_data, aes(basic,values))
basic_plot <- ggplot(basic_plot_data, aes(basic,values)) + geom_bar()
basic_plot
basic_plot <- ggplot(basic_plot_data, aes(values,basic)) + geom_bar()
basic_plot
basic_plot <- ggplot(basic_plot_data, aes(values,basic))
basic_plot +geom_bar()
basic_plot <- ggplot(basic_plot_data, aes(y = values,x =basic))
basic_plot +geom_bar()
basic_plot +geom_bar(stat = "identity")
basic_plot +geom_bar(stat = "identity") + labs(x = "")
basic_plot +geom_bar(stat = "identity") + labs(x = "", y="Count of tweets")
basic_plot +geom_bar(stat = "identity") + labs(x = "", y="count of tweets")
basic_plot +geom_bar(stat = "identity", fill='#890E1C') + labs(x = "", y="count of tweets")
View(twitter_data)
# Timeline Tweets weeksTillElection
twitter_data_timeline<-twitter_data_sample%>%select(ID, party, retweetCount, weeksTillElection)
twitter_data_timeline<-twitter_data_timeline[order(twitter_data_timeline$weeksTillElection, decreasing=TRUE),]
twitter_data_timeline<-twitter_data_timeline%>%
group_by(party, weeksTillElection)%>%
summarise(count=n(),retweetCount=sum(retweetCount))
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=retweetCount))+
geom_bar(stat="identity", fill='#890E1C')+
ylab("Anzahl Retweets")+
xlab("Wochen bis zur Wahl")
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity" ,fill='#890E1C')+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity" ,fill='#890E1C')+
ylab("count of tweets")+
xlab("Weeks until election")
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity" ,fill='#890E1C')+
ylab("count of tweets")+
xlab("weeks until election")
twitter_data_timeline_afd<-twitter_data_timeline%>%filter(party=='AfD')
plot_timeline_afd<-ggplot(twitter_data_timeline_afd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="AfD")
twitter_data_timeline_spd<-twitter_data_timeline%>%filter(party=='SPD')
plot_timeline_spd<-ggplot(twitter_data_timeline_spd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="SPD")
twitter_data_timeline_cdu<-twitter_data_timeline%>%filter(party=='CDU')
plot_timeline_cdu<-ggplot(twitter_data_timeline_cdu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="CDU")
twitter_data_timeline_csu<-twitter_data_timeline%>%filter(party=='CSU')
plot_timeline_csu<-ggplot(twitter_data_timeline_csu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="CSU")
twitter_data_timeline_linke<-twitter_data_timeline%>%filter(party=='DIE LINKE')
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
twitter_data_timeline_gruene<-twitter_data_timeline%>%filter(party=='GRUENE')
plot_timeline_gruene<-ggplot(twitter_data_timeline_gruene,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Grüne")
twitter_data_timeline_fdp<-twitter_data_timeline%>%filter(party=='FDP')
plot_timeline_fdp<-ggplot(twitter_data_timeline_fdp,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="FDP")
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_csu,
plot_timeline_linke,
plot_timeline_fdp)
plot_timeline_fdp<-ggplot(twitter_data_timeline_fdp,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="yellow")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="FDP")
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_csu,
plot_timeline_linke,
plot_timeline_fdp)
plot_timeline_gruene<-ggplot(twitter_data_timeline_gruene,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="green")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Grüne")
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="darkred")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
plot_timeline_csu<-ggplot(twitter_data_timeline_csu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="black")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="CSU")
plot_timeline_cdu<-ggplot(twitter_data_timeline_cdu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="black")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="CDU")
plot_timeline_spd<-ggplot(twitter_data_timeline_spd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="red")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="SPD")
plot_timeline_afd<-ggplot(twitter_data_timeline_afd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="lightblue")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="AfD")
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_csu,
plot_timeline_linke,
plot_timeline_fdp)
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
plot_timeline_afd<-ggplot(twitter_data_timeline_afd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="blue")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="AfD")
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="ping")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="pink")+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
geom_bar(stat="identity", fill=#FF3399)+
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill='#FF3399')+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill='#FF3399')+
ylab("Anzahl Tweets")+
xlab("Wochen bis zur Wahl")+
labs(title="Die Linke")
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
twitter_data_timeline_afd<-twitter_data_timeline%>%filter(party=='AfD')
plot_timeline_afd<-ggplot(twitter_data_timeline_afd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="blue")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="AfD")
twitter_data_timeline_spd<-twitter_data_timeline%>%filter(party=='SPD')
plot_timeline_spd<-ggplot(twitter_data_timeline_spd,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="red")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="SPD")
twitter_data_timeline_cdu<-twitter_data_timeline%>%filter(party=='CDU')
plot_timeline_cdu<-ggplot(twitter_data_timeline_cdu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="black")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="CDU")
twitter_data_timeline_csu<-twitter_data_timeline%>%filter(party=='CSU')
plot_timeline_csu<-ggplot(twitter_data_timeline_csu,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="black")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="CSU")
twitter_data_timeline_linke<-twitter_data_timeline%>%filter(party=='DIE LINKE')
plot_timeline_linke<-ggplot(twitter_data_timeline_linke,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill='#FF3399')+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="Die Linke")
twitter_data_timeline_gruene<-twitter_data_timeline%>%filter(party=='GRUENE')
plot_timeline_gruene<-ggplot(twitter_data_timeline_gruene,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="green")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="Grüne")
twitter_data_timeline_fdp<-twitter_data_timeline%>%filter(party=='FDP')
plot_timeline_fdp<-ggplot(twitter_data_timeline_fdp,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity", fill="yellow")+
ylab("count of tweets")+
xlab("weeks until election")+
labs(title="FDP")
#CSU aus Grid entfern für bessere 3x2 Darstellung, sehr geringes Tweetvolume
grid.arrange(plot_timeline_afd,
plot_timeline_gruene,
plot_timeline_spd,
plot_timeline_cdu,
plot_timeline_linke,
plot_timeline_fdp)
ggplot(twitter_data_timeline,aes(x=weeksTillElection,y=count))+
geom_bar(stat="identity" ,fill='#890E1C')+
ylab("count of tweets")+
xlab("weeks until election")
View(twitter_data_cleaned)
View(twitter_data_cleaned1)
twitter_data_sentiment<-read_delim("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_david_snd<-read_delim("data/SentimentAnalyseDavidzweiterDurchlauf.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_jakob_snd<-read_delim("twitter data/SentimentAnalyse#2.csv",
";", escape_double = FALSE, trim_ws = TRUE,
locale = locale())
View(twitter_data_sentiment)
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd,twitter_data_sentiment_david_snd)
#temp Plot für Paper
values <-c(twitter_data_sentiment%>%filter(isMatch="1"))
#temp Plot für Paper
values <-c(twitter_data_sentiment%>%filter(isMatch=="1"))
View(values)
#temp Plot für Paper
values <-c(nrow(twitter_data_sentiment%>%filter(isMatch=="1")))
values
go <-c("matched","polarity matched","not matched")
#temp Plot für Paper
values <-c(nrow(twitter_data_sentiment%>%filter(isMatch=="1")), nrow(twitter_data_sentiment%>%filter(isMatch=="0.5")))
values
#temp Plot für Paper
values <-c(nrow(twitter_data_sentiment%>%filter(isMatch=="1")), nrow(twitter_data_sentiment%>%filter(isMatch=="0.5")), nrow(twitter_data_sentiment
%>%filter(isMatch=="0")))
values
go <-c("matched","polarity matched","not matched")
basic_plot_data<-data.frame(go,values)
basic_plot <- ggplot(basic_plot_data, aes(y = values,x =go))
basic_plot +geom_bar(stat = "identity", fill='#890E1C') + labs(x = "", y="count of tweets")
#temp Plot für Paper
values <-c(nrow(twitter_data_sentiment%>%filter(isMatch=="1")), nrow(twitter_data_sentiment%>%filter(isMatch=="0")), nrow(twitter_data_sentiment
%>%filter(isMatch=="0.5")))
go <-c("matched","polarity matched","not matched")
#temp Plot für Paper
values <-c(nrow(twitter_data_sentiment%>%filter(isMatch=="1")), nrow(twitter_data_sentiment%>%filter(isMatch=="0.5")), nrow(twitter_data_sentiment
%>%filter(isMatch=="0")))
go <-c("matched","polarity matched","not matched")
values
go <-c("matched","polarity matched","not matched")
basic_plot_data<-data.frame(go,values)
basic_plot <- ggplot(basic_plot_data, aes(y = values,x =go))
basic_plot +geom_bar(stat = "identity", fill='#890E1C') + labs(x = "", y="count of tweets")
colnames(twitter_data_sentiment_david_snd)<-c("sentimentScore","text")
twitter_data_sentiment<-twitter_data_sentiment%>%select(-c(ID))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%select(-c(X1,ID,positveSentimentScore,negativeSentimentScore))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%filter(sentimentScore != "NA")
twitter_data_sentiment_jakob_snd$isMatch<-"1"
twitter_data_sentiment_david_snd$isMatch<-"1"
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd,twitter_data_sentiment_david_snd)
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))
# Umwandeln der Scores in numeric mit "."
twitter_data_sentiment<-twitter_data_sentiment%>%filter(sentimentScore != "NA")
twitter_data_sentiment<-twitter_data_sentiment%>%filter(text != "")
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(sentimentScore = sub(",", ".", sentimentScore, fixed = TRUE))
twitter_data_sentiment$sentimentScore<-as.numeric(as.character(twitter_data_sentiment$sentimentScore))
>>>>>>> 55cc94dd0e279e6579ba715551e2f945fb3f362d
