text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("ã","ä",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
return(text)
}
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]#|@]",replacement=" ")
return(text)
}
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-twitter_data_cleaned$text<-gsub("<[^\\s]+>","",twitter_data_cleaned$text)
# Stemming
twitter_data_cleaned$text<-apply(twitter_data_cleaned[,"text"],1,function(x) stemTweetText(x))
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("ã","ä",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
return(text)
}
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]#|@]",replacement=" ")
return(text)
}
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
predict<-predict(lasso_5lvl,twitter_data_cleaned$text)
twitter_data_cleaned$text<-twitter_data_cleaned$text<-iconv(twitter_data_cleaned$text, 'UTF-8','ASCII')
predict<-predict(lasso_5lvl,twitter_data_cleaned$text)
View(predict)
twitter_data$lasso5<-predict
View(twitter_data)
View(predict)
as.numeric(unlist(predict))
View(predict)
twitter_data$lasso5<-as.numeric(unlist(predict))
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv")
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUe)
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUE)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("ã","ä",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
return(text)
}
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement="")
return(text)
}
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
twitter_data_cleaned$text<-twitter_data_cleaned$text<-iconv(twitter_data_cleaned$text, 'UTF-8','ASCII')
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
# Stemming
twitter_data_cleaned$text<-apply(twitter_data_cleaned[,"text"],1,function(x) stemTweetText(x))
twitter_data$lasso5<-""
twitter_data$lasso5<-as.numeric(unlist(predict))
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUE)
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUE)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("ã","ä",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
return(text)
}
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
return(text)
}
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
twitter_data_cleaned$text<-twitter_data_cleaned$text<-iconv(twitter_data_cleaned$text, 'UTF-8','ASCII')
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
# Stemming
twitter_data_cleaned$text<-apply(twitter_data_cleaned[,"text"],1,function(x) stemTweetText(x))
twitter_data$lasso5<-as.numeric(unlist(predict))
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("ã","ä",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
return(text)
}
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
return(text)
}
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
twitter_data_cleaned$text<-twitter_data_cleaned$text<-iconv(twitter_data_cleaned$text, 'UTF-8','ASCII')
twitter_data_cleaned<-twitter_data_cleaned%>%mutate(text=cleanTweetText(text))
# Stemming
twitter_data_cleaned$text<-apply(twitter_data_cleaned[,"text"],1,function(x) stemTweetText(x))
twitter_data$lasso5<-as.numeric(unlist(predict))
twitter_data<-twitter_data%>%mutate(lasso5=round(lasso5, 5))
write.csv(twitter_data, "twitter data/twitterDataWithSentiment.csv", row.names = FALSE, col.names = TRUE)
predict<-predict(ridge_5lvl,twitter_data_cleaned$text)
twitter_data1<-twitter_data[1:120000,]
twitter_data_cleaned1<-twitter_data_cleaned[1:120000,]
predict1<-predict(ridge_5lvl,twitter_data_cleaned1$text)
