#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTextForMerge("<U+6594>")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data_sentiment<-read_delim("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%select(-c(X1,ID,positveSentimentScore,negativeSentimentScore))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%filter(sentimentScore != "NA")
twitter_data_sentiment_jakob_snd$isMatch<-"1"
twitter_data_sentiment<-twitter_data_sentiment%>%select(-c(ID))
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd)
twitter_data_sentiment<-twitter_data_sentiment%>%filter(isMatch=="1")
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))
twitter_data_sentiment<-twitter_data_sentiment%>%filter(text!="")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data<-twitter_data%>%filter(text != "")
twitter_data_sentiment22<-join(x=twitter_data_sentiment,y=twitter_data, type = "inner")
twitter_data_sentiment22<-twitter_data_sentiment22[!duplicated((twitter_data_sentiment22$text)),]
twitter_data_sentiment_anti<-anti_join(x=twitter_data_sentiment,y=twitter_data)
View(twitter_data_sentiment_anti)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTweetText(".@c_lindner ungeschminkt. Christian #Lindner Unterhemd.https://t.co/")
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("\U00E3","ä",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
text<-gsub("\\s+", " ",text)
return(text)
}
cleanTextForMerge<-function(text){
mention_pattern <- "@([[:alnum:]]|[_])+"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-gsub("\U2026","",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
text<-substr(text, 1, 30)
return(text)
}
cleanTextForMerge("<U+6594>")
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data_sentiment<-read_delim("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE,
locale = locale())
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%select(-c(X1,ID,positveSentimentScore,negativeSentimentScore))
twitter_data_sentiment_jakob_snd<-twitter_data_sentiment_jakob_snd%>%filter(sentimentScore != "NA")
twitter_data_sentiment_jakob_snd$isMatch<-"1"
twitter_data_sentiment<-twitter_data_sentiment%>%select(-c(ID))
twitter_data_sentiment<-rbind(twitter_data_sentiment,twitter_data_sentiment_jakob_snd)
twitter_data_sentiment<-twitter_data_sentiment%>%filter(isMatch=="1")
twitter_data_sentiment<-twitter_data_sentiment%>%mutate(text=cleanTweetText(text))
twitter_data_sentiment<-twitter_data_sentiment%>%filter(text!="")
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
twitter_data<-twitter_data%>%mutate(text=cleanTextForMerge(text))
twitter_data<-twitter_data%>%filter(text != "")
twitter_data_sentiment22<-join(x=twitter_data_sentiment,y=twitter_data, type = "inner")
twitter_data_sentiment22<-twitter_data_sentiment22[!duplicated((twitter_data_sentiment22$text)),]
View(twitter_data_sentiment_anti)
cleanTweetText("gute amp richtige entscheidung")
Encoding(twitter_data_sentiment_anti[1,])
twitter_data_sentiment_anti[1,]
Encoding(twitter_data_sentiment_anti[1,3])
twitter_data_sentiment_anti[1,3]
toxt<-twitter_data_sentiment_anti[1,3]
Encoding(toxt)
Encoding(as.character(toxt))
twitter_data_sentiment<-read_csv2("twitter data/SentimentAnalyse#1.csv",";"
,escape_double = FALSE, trim_ws =TRUE, encoding="UTF-8")
twitter_data_sentiment<-read_csv2("twitter data/SentimentAnalyse#1.csv",";", encoding="UTF-8")
twitter_data_sentiment<-read.csv2("twitter data/SentimentAnalyse#1.csv",";", encoding="UTF-8")
source("r scripts/cleanTweetText.R")
require(tidyverse)
require(dplyr)
require(plyr)
library(readr)
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,ID,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
twitter_data_sentiment22<-twitter_data_sentiment22%>%select(c(ID,sentimentScore))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_sentiment22)
View(twitter_data2)
twitter_data<-read_delim("twitter data/twitter_data.csv",
",", escape_double = FALSE, trim_ws = TRUE,
locale = locale())
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
View(twitter_data2)
twitter_data2<-read_delim("twitter data/twitter_data.csv",
",", escape_double = FALSE, trim_ws = TRUE,
locale = locale())
twitter_data2<-twitter_data2%>%select(-c(isRetweet,isQuote,inReplyToUser,lang,retweetCount,favouriteCount,hashtagCount,mentionCount, inReplyToStatus,isSourceTweet,userID,createdAT,insertedAT,charCount,tokenCount,urlCount))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
lasso_5lvl<-read("dictionarys/lasso-5level.dict")
lasso_3lvl<-read("dictionarys/lasso-3level.dict")
lasso_2lvl<-read("dictionarys/lasso-2level.dict")
ridge_5lvl<-read("dictionarys/ridge-5level.dict")
ridge_3lvl<-read("dictionarys/ridge-3level.dict")
ridge_2lvl<-read("dictionarys/ridge-2level.dict")
enet_5lvl<-read("dictionarys/enet-5level.dict")
enet_3lvl<-read("dictionarys/enet-3level.dict")
enet_2lvl<-read("dictionarys/enet-2level.dict")
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
View(twitter_data_WS_join)
cleanTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
hashtag_pattern <- "#([[:alnum:]]|[_])+"
mention_pattern <- "@([[:alnum:]]|[_])+"
strip_RT_pattern<-"RT\\s@([[:alnum:]]|[_])+:"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-ifelse(grepl("\U2026",text),"",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
#text<-substr(text, 1, 30)
return(text)
}
cleanTweetText("gute amp richtige entscheidung")
stemTweetText<-function(text){
require(stringr)
require(stringi)
library(tm)
library(SnowballC)
text<-as.character(text)
text<-strsplit(text," ")
words<-unlist(text)
words<-words[words!=""]
words<-wordStem(words,language="german")
words<-paste(words,collapse=" ")
return(words)
}
cleanCorpus<-function(text){
require(stringr)
require(stringi)
text<-gsub("<e4>","ä",text)
text<-gsub("<c4>","Ä",text)
text<-gsub("<d6>","Ö",text)
text<-gsub("<dc>","Ü",text)
text<-gsub("<f6>","ö",text)
text<-gsub("<fc>","ü",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("\U00E3","ä",text)
text<-gsub("&amp;","",text)
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
text<-str_replace_all(text,pattern="[^[:alnum:]\\#|\\@]",replacement=" ")
text<-gsub("\\s+", " ",text)
return(text)
}
cleanTextForMerge<-function(text){
mention_pattern <- "@([[:alnum:]]|[_])+"
text<-gsub("<e4>","ae",text)
text<-gsub("<c4>","Ae",text)
text<-gsub("<d6>","Oe",text)
text<-gsub("<dc>","UE",text)
text<-gsub("<f6>","oe",text)
text<-gsub("<fc>","ue",text)
text<-gsub("<df>","ss",text)
text<-gsub("ß","ss",text)
text<-gsub("\U00DF","ss",text)
text<-gsub("\U00E3","ae",text)
text<-gsub("<[^\\s]+>","",text)
text<-gsub("<[^\\s]+","",text)
text<-gsub("&amp;","",text)
text<-gsub(" amp "," ",text)
text<-gsub("http[s]?\\://t\\.co/[^ ]{10}","",text)
text<-gsub("http.*[^\\s]+","",text)
text<-gsub("htt[p]?\U2026","",text)
text<-gsub("https","",text)
text<-gsub("http","",text)
text<-gsub("\U2026","",text)
#Die URLs und Mentions werden entfernt
text<-str_replace_all(text,pattern=mention_pattern,replacement="")
#Wörter die weniger als 3 Zeichen haben müssen weg außer sie haben #
text<-str_replace_all(text,pattern="(?<!#)\\b[a-zA-Z0-9]{1,2}\\b",replacement = "")
#depends on whether you wanna keep hashtags or not
text<-str_replace_all(text,pattern="#",replacement="")
#Konvertierung von Umlauten
text<-stri_replace_all_fixed(text,
#c("?", "?", "?", "?", "?", "?"),
c("\U00E4","\U00F6","\U00FC","\U00C4","\U00D6","\U00DC"),
c("ae", "oe", "ue", "Ae", "Oe", "Ue"), vectorize_all = FALSE)
#Zahlen außer IN Hashtags
text<-str_replace_all(text,pattern="\\b\\d+\\b",replacement="")
#Satzzeichen und Special Characters außer # müssen weg
text<-str_replace_all(text,pattern="[^[:alnum:]#]",replacement=" ")
text<-str_to_lower(text)
text<-gsub("\\s+", " ",text)
text<-sub("\\s+$", "", text)
#text<-substr(text, 1, 30)
return(text)
}
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
View(twitter_data_WS_join)
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
View(twitter_data_WS_join)
twitter_data_WS_join_text<-twitter_data_WS_join$text
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join_text<-twitter_data_WS_join[,"text"]
twitter_data_WS_join_text$text<-twitter_data_WS_join[,"text"]
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-apply(twitter_data_WS_join_text$text,1,function(x) stemTweetText(x))
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
View(twitter_data_WS_join)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-lapply(twitter_data_WS_join_text$text,1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],function(x) stemTweetText(x))
lasso5<-predict(lasso_5lvl,twitter_data_WS_join$text)
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
View(twitter_data_WS_join)
require(tidyverse)
require(dplyr)
library(readr)
library(SentimentAnalysis)
lasso_5lvl<-read("dictionarys/lasso-5level.dict")
lasso_3lvl<-read("dictionarys/lasso-3level.dict")
lasso_2lvl<-read("dictionarys/lasso-2level.dict")
ridge_5lvl<-read("dictionarys/ridge-5level.dict")
ridge_3lvl<-read("dictionarys/ridge-3level.dict")
ridge_2lvl<-read("dictionarys/ridge-2level.dict")
enet_5lvl<-read("dictionarys/enet-5level.dict")
enet_3lvl<-read("dictionarys/enet-3level.dict")
enet_2lvl<-read("dictionarys/enet-2level.dict")
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
twitter_data_WS_join<-as.data.frame(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join<-join(x=twitter_data_sentiment22,y=twitter_data2, type = "inner", by="ID")
twitter_data_WS_join<-as.data.frame(twitter_data_WS_join)
twitter_data_WS_join$text<-gsub("<[^\\s]+>","",twitter_data_WS_join$text)
twitter_data_WS_join<-twitter_data_WS_join%>%mutate(text=cleanTweetText(text))
twitter_data_WS_join<-twitter_data_WS_join%>%filter(text!="")
twitter_data_WS_join$text<-apply(twitter_data_WS_join[,"text"],1,function(x) stemTweetText(x))
twitter_data_WS_join$text<-lapply(twitter_data_WS_join[,"text"],function(x) stemTweetText(x))
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,twitter_data_WS_join$text)))
twitter_data_WS_join$enet5<-as.numeric(unlist(predict(enet_5lvl,unlist(twitter_data_WS_join$text))))
predict(enet_5lvl,"vorsprung durch leckmich kartell")
View(twitter_data_WS)
as.numeric(predict(enet_5lvl,"vorsprung durch leckmich kartell"))
twitter_data_WS<-twitter_data_WS%>%select(-c(isRetweet,isQuote))
View(twitter_data2)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
View(twitter_data)
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
View(twitter_data)
twitter_data<-twitter_data%>%select(c(text,ID))
View(twitter_data)
twitter_data_WS<-join(x=twitter_data_WS,y=twitter_data,type="inner",by="text")
View(twitter_data_WS)
library(readr)
twitterDataWithSentiment <- read_delim("D:/GitHub/CSS-WS17-18/twitter data/twitterDataWithSentiment.csv",
";", escape_double = FALSE, locale = locale(),
trim_ws = TRUE)
View(twitterDataWithSentiment)
twitter_data<-read_csv("twitter data/twitter_data.csv",
locale = locale())
# Aufbereiten Twitter Datensatz
twitter_data<-twitter_data_cleaned<-twitter_data%>%filter(lang=="de" | lang == "da")
#Entfernt alle Links (http(s) und alle Zeichen bis zum nächsten Leerzeichen)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http[s]?://t\\.co/[^ ]{10}","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http.*[^\\s]+","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("https","",twitter_data_cleaned$text)
twitter_data$text<-twitter_data_cleaned$text<-gsub("http","",twitter_data_cleaned$text)
#Entfernt alle abgeschnittenen Links
twitter_data$text<-twitter_data_cleaned$text<-gsub("htt[p]?\U2026","",twitter_data$text)
#Entfernen von abgeschnittenen Tweets
twitter_data<-twitter_data_cleaned<-twitter_data[!grepl("\U2026", twitter_data$text),]
#CleanTweetText auf den gesamten Twitter Datensatz
twitter_data<-twitter_data%>%mutate(text=cleanCorpus(text))
twitter_data$text<-gsub("<[^\\s]+>","",twitter_data$text)
View(twitter_data)
View(twitterDataWithSentiment)
twitterDataWithSentiment$ID<-twitter_data$ID
moveme(names(twitterDataWithSentiment),"ID first")
twitterDataWithSentiment<-twitterDataWithSentiment%>%select(ID,everything())
View(twitterDataWithSentiment)
View(twitter_data_WS_join)
write.csv(twitter_data_WS_join, "twitter data/twitterDataMatches.csv",sep=",", col.names = TRUE, row.names = FALSE)
write.table(twitter_data_WS_join, "twitter data/twitterDataMatches.csv", sep=",",col.names = TRUE, row.names = FALSE)
str(twitter_data_WS_join)
twitter_data_WS_join$text<-unlist(twitter_data_WS_join$text)
write.table(twitter_data_WS_join, "twitter data/twitterDataMatches.csv", sep=",",col.names = TRUE, row.names = FALSE)
twitter_data_WS_join<-twitter_data_WS_join%>%select(ID,sentimentScore)
twitterDataWithSentiment2<-join(x=twitterDataWithSentiment,twitter_data_WS_join,type="inner",by="ID")
twitterDataWithSentiment2<-join(x=twitterDataWithSentiment,twitter_data_WS_join,type="left",by="ID")
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
View(twitterDataWithSentiment2)
twitterDataWS<-twitterDataWithSentiment2%>%filter(sentimentScore!="NA")
View(twitterDataWS)
twitterDataWS<-twitterDataWS[!duplicated((twitterDataWS$text)),]
write.table(twitterDataWS, "twitter data/twitterDataBaselineWithAllDicts.csv", sep=",",col.names = TRUE, row.names = FALSE)
